{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_url(text): \n",
    "    url_pattern  = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    return url_pattern.sub(r'', text)\n",
    " # converting return value from list to string\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text ): \n",
    "    delete_dict = {sp_character: '' for sp_character in string.punctuation} \n",
    "    delete_dict[' '] = ' ' \n",
    "    table = str.maketrans(delete_dict)\n",
    "    text1 = text.translate(table)\n",
    "    #print('cleaned:'+text1)\n",
    "    textArr= text1.split()\n",
    "    text2 = ' '.join([w for w in textArr if ( not w.isdigit() and  ( not w.isdigit() and len(w)>2))]) \n",
    "    \n",
    "    return text2.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(sentiment):\n",
    "    if sentiment == 'positive':\n",
    "        return 2\n",
    "    elif sentiment == 'negative':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Train data--------\n",
      "neutral     10704\n",
      "positive     8375\n",
      "negative     7673\n",
      "Name: sentiment, dtype: int64\n",
      "26752\n",
      "-------------------------\n",
      "-------Test data--------\n",
      "neutral     1376\n",
      "positive    1075\n",
      "negative     983\n",
      "Name: sentiment, dtype: int64\n",
      "3434\n",
      "-------------------------\n",
      "Train Max Sentence Length :33\n",
      "Test Max Sentence Length :32\n"
     ]
    }
   ],
   "source": [
    "train_data= pd.read_csv(\"train.csv\")\n",
    "train_data.dropna(axis = 0, how ='any',inplace=True) \n",
    "train_data['Num_words_text'] = train_data['text'].apply(lambda x:len(str(x).split())) \n",
    "mask = train_data['Num_words_text'] >2\n",
    "train_data = train_data[mask]\n",
    "print('-------Train data--------')\n",
    "print(train_data['sentiment'].value_counts())\n",
    "print(len(train_data))\n",
    "print('-------------------------')\n",
    "max_train_sentence_length  = train_data['Num_words_text'].max()\n",
    "\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(remove_emoji)\n",
    "train_data['text'] = train_data['text'].apply(remove_url)\n",
    "train_data['text'] = train_data['text'].apply(clean_text)\n",
    "\n",
    "train_data['label'] = train_data['sentiment'].apply(get_sentiment)\n",
    "\n",
    "test_data= pd.read_csv(\"test.csv\")\n",
    "test_data.dropna(axis = 0, how ='any',inplace=True) \n",
    "test_data['Num_words_text'] = test_data['text'].apply(lambda x:len(str(x).split())) \n",
    "\n",
    "max_test_sentence_length  = test_data['Num_words_text'].max()\n",
    "\n",
    "mask = test_data['Num_words_text'] >2\n",
    "test_data = test_data[mask]\n",
    "\n",
    "print('-------Test data--------')\n",
    "print(test_data['sentiment'].value_counts())\n",
    "print(len(test_data))\n",
    "print('-------------------------')\n",
    "\n",
    "test_data['text'] = test_data['text'].apply(remove_emoji)\n",
    "test_data['text'] = test_data['text'].apply(remove_url)\n",
    "test_data['text'] = test_data['text'].apply(clean_text)\n",
    "\n",
    "test_data['label'] = test_data['sentiment'].apply(get_sentiment)\n",
    "\n",
    "print('Train Max Sentence Length :'+str(max_train_sentence_length))\n",
    "print('Test Max Sentence Length :'+str(max_test_sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Num_words_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>have responded were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>sooo sad will miss you here san diego</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>boss bullying</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview leave alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>sons why couldnt they put them the releases al...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28b57f3990</td>\n",
       "      <td>some shameless plugging for the best rangers f...</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when all smi...</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e050245fbd</td>\n",
       "      <td>both you</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fc2cbefa9d</td>\n",
       "      <td>journey wow just became cooler hehe that possible</td>\n",
       "      <td>Wow... u just became cooler.</td>\n",
       "      <td>positive</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2339a9b08b</td>\n",
       "      <td>much love hopeful reckon the chances are minim...</td>\n",
       "      <td>as much as i love to be hopeful, i reckon the ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        textID                                               text  \\\n",
       "0   cb774db0d1                          have responded were going   \n",
       "1   549e992a42              sooo sad will miss you here san diego   \n",
       "2   088c60f138                                      boss bullying   \n",
       "3   9642c003ef                         what interview leave alone   \n",
       "4   358bd9e861  sons why couldnt they put them the releases al...   \n",
       "5   28b57f3990  some shameless plugging for the best rangers f...   \n",
       "6   6e0c6d75b1  2am feedings for the baby are fun when all smi...   \n",
       "8   e050245fbd                                           both you   \n",
       "9   fc2cbefa9d  journey wow just became cooler hehe that possible   \n",
       "10  2339a9b08b  much love hopeful reckon the chances are minim...   \n",
       "\n",
       "                                        selected_text sentiment  \\\n",
       "0                 I`d have responded, if I were going   neutral   \n",
       "1                                            Sooo SAD  negative   \n",
       "2                                         bullying me  negative   \n",
       "3                                      leave me alone  negative   \n",
       "4                                       Sons of ****,  negative   \n",
       "5   http://www.dothebouncy.com/smf - some shameles...   neutral   \n",
       "6                                                 fun  positive   \n",
       "8                                         Both of you   neutral   \n",
       "9                        Wow... u just became cooler.  positive   \n",
       "10  as much as i love to be hopeful, i reckon the ...   neutral   \n",
       "\n",
       "    Num_words_text  label  \n",
       "0                7      0  \n",
       "1               10      1  \n",
       "2                5      1  \n",
       "3                5      1  \n",
       "4               14      1  \n",
       "5               12      0  \n",
       "6               14      2  \n",
       "8                3      0  \n",
       "9               10      2  \n",
       "10              23      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Num_words_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>last session the day</td>\n",
       "      <td>neutral</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>shanghai also really exciting precisely skyscr...</td>\n",
       "      <td>positive</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>recession hit veronique branquinho she has qui...</td>\n",
       "      <td>negative</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>like</td>\n",
       "      <td>positive</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>726e501993</td>\n",
       "      <td>thats great weee visitors</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>261932614e</td>\n",
       "      <td>think everyone hates here lol</td>\n",
       "      <td>negative</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>afa11da83f</td>\n",
       "      <td>soooooo wish could but school and myspace comp...</td>\n",
       "      <td>negative</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e64208b4ef</td>\n",
       "      <td>and within short time the last clue all them</td>\n",
       "      <td>neutral</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>37bcad24ca</td>\n",
       "      <td>what did you get day alright havent done anyth...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>24c92644a4</td>\n",
       "      <td>bike was put holdshould have known that argh t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        textID                                               text sentiment  \\\n",
       "0   f87dea47db                               last session the day   neutral   \n",
       "1   96d74cb729  shanghai also really exciting precisely skyscr...  positive   \n",
       "2   eee518ae67  recession hit veronique branquinho she has qui...  negative   \n",
       "4   33987a8ee5                                               like  positive   \n",
       "5   726e501993                          thats great weee visitors  positive   \n",
       "6   261932614e                      think everyone hates here lol  negative   \n",
       "7   afa11da83f  soooooo wish could but school and myspace comp...  negative   \n",
       "8   e64208b4ef       and within short time the last clue all them   neutral   \n",
       "9   37bcad24ca  what did you get day alright havent done anyth...   neutral   \n",
       "10  24c92644a4  bike was put holdshould have known that argh t...  negative   \n",
       "\n",
       "    Num_words_text  label  \n",
       "0                6      0  \n",
       "1               15      2  \n",
       "2               13      1  \n",
       "4                5      2  \n",
       "5                4      2  \n",
       "6                8      1  \n",
       "7               13      1  \n",
       "8               12      0  \n",
       "9               18      0  \n",
       "10              12      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data len:21401\n",
      "Class distributionCounter({0: 8563, 2: 6700, 1: 6138})\n",
      "Valid data len:5351\n",
      "Class distributionCounter({0: 2141, 2: 1675, 1: 1535})\n",
      "Test data len:3434\n",
      "Class distributionCounter({0: 1376, 2: 1075, 1: 983})\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, Y_train, Y_valid= train_test_split(train_data['text'].tolist(),\\\n",
    "                                                      train_data['label'].tolist(),\\\n",
    "                                                      test_size=0.2,\\\n",
    "                                                      stratify = train_data['label'].tolist(),\\\n",
    "                                                      random_state=0)\n",
    "\n",
    "\n",
    "print('Train data len:'+str(len(X_train)))\n",
    "print('Class distribution'+str(Counter(Y_train)))\n",
    "\n",
    "\n",
    "print('Valid data len:'+str(len(X_valid)))\n",
    "print('Class distribution'+ str(Counter(Y_valid)))\n",
    "\n",
    "print('Test data len:'+str(len(test_data['text'].tolist())))\n",
    "print('Class distribution'+ str(Counter(test_data['label'].tolist())))\n",
    "\n",
    "\n",
    "train_dat =list(zip(Y_train,X_train))\n",
    "valid_dat =list(zip(Y_valid,X_valid))\n",
    "test_dat=list(zip(test_data['label'].tolist(),test_data['text'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ciner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = train_dat\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[62, 0, 1, 0, 12881]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline('here is the an example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pipeline('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "         label_list.append(label_pipeline(_label))\n",
    "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "         offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "#train_iter =train_dat\n",
    "#dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc1 = nn.Linear(embed_dim,64)\n",
    "        self.fc2 = nn.Linear(64,16)\n",
    "        self.fc3 = nn.Linear(16, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.bias.data.zero_()\n",
    "        self.fc3.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc3.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        x = F.relu(self.fc1(embedded))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "train_iter1 = train_dat\n",
    "num_class = len(set([label for (label, text) in train_iter1]))\n",
    "print(num_class)\n",
    "vocab_size = len(vocab)\n",
    "emsize = 128\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predited_label = model(text, offsets)\n",
    "        loss = criterion(predited_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predited_label = model(text, offsets)\n",
    "            loss = criterion(predited_label, label)\n",
    "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1338 batches | accuracy    0.449\n",
      "| epoch   1 |  1000/ 1338 batches | accuracy    0.540\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  6.79s | valid accuracy    0.589 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1338 batches | accuracy    0.617\n",
      "| epoch   2 |  1000/ 1338 batches | accuracy    0.623\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  3.99s | valid accuracy    0.616 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1338 batches | accuracy    0.667\n",
      "| epoch   3 |  1000/ 1338 batches | accuracy    0.670\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  4.28s | valid accuracy    0.627 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1338 batches | accuracy    0.698\n",
      "| epoch   4 |  1000/ 1338 batches | accuracy    0.707\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  4.25s | valid accuracy    0.651 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1338 batches | accuracy    0.736\n",
      "| epoch   5 |  1000/ 1338 batches | accuracy    0.726\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  4.19s | valid accuracy    0.670 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1338 batches | accuracy    0.761\n",
      "| epoch   6 |  1000/ 1338 batches | accuracy    0.743\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  4.08s | valid accuracy    0.659 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1338 batches | accuracy    0.807\n",
      "| epoch   7 |  1000/ 1338 batches | accuracy    0.824\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  3.93s | valid accuracy    0.686 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1338 batches | accuracy    0.837\n",
      "| epoch   8 |  1000/ 1338 batches | accuracy    0.839\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  4.17s | valid accuracy    0.686 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1338 batches | accuracy    0.856\n",
      "| epoch   9 |  1000/ 1338 batches | accuracy    0.855\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  4.26s | valid accuracy    0.683 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1338 batches | accuracy    0.871\n",
      "| epoch  10 |  1000/ 1338 batches | accuracy    0.872\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  4.21s | valid accuracy    0.684 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR =10  # learning rate\n",
    "BATCH_SIZE = 16 # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "\n",
    "train_iter2 = train_dat\n",
    "test_iter2 =test_dat \n",
    "valid_iter2= valid_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_iter2, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(valid_iter2, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_iter2, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.679\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Positive tweet\n"
     ]
    }
   ],
   "source": [
    "sentiment_label = {2:\"Positive\",\n",
    "                   1: \"Negative\",\n",
    "                   0: \"Neutral\"\n",
    "                  }\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() \n",
    "ex_text_str = \"Happy Birthday Snickers!!!! ? I hope you have the best day ever! Let`s go shopping!!!\"\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is a %s tweet\" %sentiment_label[predict(ex_text_str, text_pipeline)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1307,  0.0202, -0.1635,  ...,  0.4103, -0.1344,  0.3930],\n",
      "        [-0.1276, -0.0887,  0.2664,  ..., -0.0921,  0.2704,  0.0501],\n",
      "        [-0.0218, -0.1828, -0.1208,  ...,  0.2758, -0.2773,  0.0977],\n",
      "        ...,\n",
      "        [ 0.2968, -0.3142, -0.3402,  ...,  0.4607,  0.0584, -0.5102],\n",
      "        [-0.1291, -0.0230,  0.3256,  ..., -0.4332,  0.4607, -0.0858],\n",
      "        [ 0.0022, -0.4016, -0.0677,  ..., -0.3719, -0.1085, -0.4813]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-4.1814e-01,  2.6218e-01, -3.5425e-01,  ..., -9.6711e-02,\n",
      "          1.2056e-01,  2.3817e-01],\n",
      "        [-6.3517e-01,  3.2474e-01,  2.6492e-01,  ...,  1.7044e-01,\n",
      "          7.1621e-04, -8.2756e-02],\n",
      "        [-3.4442e-01, -8.5487e-01,  1.2719e+00,  ..., -1.3776e-01,\n",
      "          4.5986e-01, -1.2128e-01],\n",
      "        ...,\n",
      "        [-3.2299e-01,  4.4040e-01,  5.7413e-01,  ...,  2.8853e-01,\n",
      "         -3.6174e-01, -3.8113e-01],\n",
      "        [-2.2013e-01, -1.1470e+00,  6.8965e-01,  ..., -5.6169e-01,\n",
      "         -1.1362e-03,  7.2397e-01],\n",
      "        [ 3.8260e-01,  1.3156e-01,  3.5829e-01,  ...,  4.4228e-01,\n",
      "         -2.6486e-01,  1.9746e-01]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-1.3080, -1.2071, -1.1482, -0.9056, -0.8094, -0.4777, -0.9311, -1.6734,\n",
      "        -0.9601, -0.9979, -0.9705, -1.1115, -0.7807, -1.1593, -0.9629, -0.9916,\n",
      "        -1.4999, -1.0379, -1.2085, -0.3453, -0.3144, -1.2396, -0.6000, -0.3052,\n",
      "         0.1729, -0.5310, -1.2079, -0.8157, -0.5422, -1.1807, -0.6980, -1.1329,\n",
      "        -0.6395, -1.0612, -1.1475, -0.1856, -0.1089, -1.0715, -0.6855, -1.0261,\n",
      "        -0.5930, -1.5148, -0.6654, -0.8508, -1.2186, -0.5613, -1.1224, -1.0563,\n",
      "        -0.9670, -0.5576, -0.7913, -0.7642, -0.6156, -0.7803, -0.7388, -0.0957,\n",
      "        -0.9199, -1.0514, -0.7570, -1.1985, -0.7205, -1.1103, -0.3785, -0.6258],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2024, -0.2916, -0.4883,  ..., -0.0925, -0.5154, -0.3341],\n",
      "        [ 0.7200, -0.6290, -0.2059,  ...,  0.0800, -0.3150,  0.0731],\n",
      "        [ 0.2488, -0.3364, -0.5031,  ...,  0.2230, -0.4318, -0.0781],\n",
      "        ...,\n",
      "        [-0.4931,  0.2734, -0.1278,  ..., -0.5093,  0.0098,  0.4409],\n",
      "        [-0.2925,  0.0669, -0.1744,  ..., -0.2348, -0.5234, -0.0572],\n",
      "        [ 0.3185,  0.4590,  0.1094,  ..., -0.4655,  0.0048,  0.2812]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-1.0860, -0.5542, -0.7817, -0.8832,  0.7826,  0.0072, -0.5913, -0.4355,\n",
      "        -0.3809,  0.7693, -0.3527, -0.7673, -1.5236, -0.6484, -0.4067, -0.6475],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 3.1061e-01, -4.0621e-01, -2.6115e-02,  6.5084e-01,  2.3781e-01,\n",
      "          3.7583e-01,  4.9943e-01, -8.0740e-01,  6.2858e-01, -7.6889e-01,\n",
      "          4.7298e-01, -2.4114e-01,  4.7686e-01,  3.6960e-01,  6.8016e-01,\n",
      "          1.9761e-01],\n",
      "        [-6.8071e-01,  2.5703e-03,  4.4863e-01, -3.1354e-01, -1.1977e+00,\n",
      "          3.0874e-01, -1.6580e-01,  1.4338e+00, -9.8500e-01, -1.1724e+00,\n",
      "          3.4780e-02,  1.8395e-01,  2.9320e-01,  3.0006e-01,  1.4544e-01,\n",
      "         -8.6266e-05],\n",
      "        [ 3.2802e-02, -2.0896e-01, -5.3235e-01, -4.4086e-01,  1.0619e+00,\n",
      "         -6.6427e-01, -9.5029e-01, -7.6444e-01, -1.9542e-01,  1.6558e+00,\n",
      "         -2.6240e-01,  3.4481e-01, -4.1285e-01, -1.7392e-01, -8.7469e-02,\n",
      "         -2.9119e-01]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.7511,  0.2619, -1.0131], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = \"model.pth\"\n",
    "torch.save(model, FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Positive tweet\n"
     ]
    }
   ],
   "source": [
    "FILE = \"model.pth\"\n",
    "\n",
    "model = torch.load(FILE)\n",
    "model.eval()\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     print(param)\n",
    "\n",
    "sentiment_label = {2:\"Positive\",\n",
    "                   1: \"Negative\",\n",
    "                   0: \"Neutral\"\n",
    "                  }\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() \n",
    "ex_text_str = \"Happy Birthday Snickers!!!! ? I hope you have the best day ever! Let`s go shopping!!!\"\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is a %s tweet\" %sentiment_label[predict(ex_text_str, text_pipeline)])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "acfd08a7faa67adda30a139a14258a4609e125be16bd9d56539cb9e1d9f3c0a6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
